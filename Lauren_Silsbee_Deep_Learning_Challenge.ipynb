{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lauren Silsbee: Deep Learning Challenge.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMYpQIQZroCgq145q/0PqrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrsilsbee/Machine-Learning/blob/main/Lauren_Silsbee_Deep_Learning_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "LJagJ9jbpY_C",
        "outputId": "bac0f99b-f209-47ad-b4c8-58c705d9c940"
      },
      "source": [
        "pip install tensorflow-gpu==2.0.0-rc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/cf/2fc69ba3e59edc8333e2676fa71b40197718dea7dc1282c79955cf6b2acb/tensorflow_gpu-2.0.0rc1-cp36-cp36m-manylinux2010_x86_64.whl (380.5MB)\n",
            "\u001b[K     |████████████████████████████████| 380.5MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.19.4)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-rc1) (51.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.7.4.3)\n",
            "Installing collected packages: keras-applications, tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed keras-applications-1.0.8 tb-nightly-1.15.0a20190806 tensorflow-gpu-2.0.0rc1 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgX-VJMOpGEQ"
      },
      "source": [
        "#imports\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "from tensorflow.keras.datasets import fashion_mnist\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential \r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdLtFX0-pdMs"
      },
      "source": [
        "1. Preprocess your data so that you can feed it into ANN models.\r\n",
        "2. Split your data into training and test sets.\r\n",
        "3. Compare your models' training scores and interpret your results.\r\n",
        "4. Evaluate your models' performances on your test set.\r\n",
        "5. Compare the results of your models.\r\n",
        "\r\n",
        "The Fashion MNIST dataset is very similar to the MNIST dataset, as it has 60,000 training observations, 10,000 testing observations, 10 classes, and then 28x28 images. So, the preprocessing can be similar to the preprocessing steps used for the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V36p-6Xxp0EF",
        "outputId": "ccdb3da3-3986-4afe-e4e0-3b56edf014b0"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "#Fashion MNIST has 28x28 greyscale images, so 28x28 = 784\r\n",
        "input_dim = 784  \r\n",
        "#10 classes:\r\n",
        "output_dim = nb_classes = 10\r\n",
        "#set 20 epochs\r\n",
        "nb_epoch = 20\r\n",
        "\r\n",
        "#split into test and training sets\r\n",
        "X_train = X_train.reshape(60000, input_dim)\r\n",
        "X_test = X_test.reshape(10000, input_dim)\r\n",
        "#convert to float type\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "#normalize vectors\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vuCg1r7p5l8"
      },
      "source": [
        "#convert Y_train, Y_test to categorical type\r\n",
        "Y_train = to_categorical(y_train, nb_classes)\r\n",
        "Y_test = to_categorical(y_test, nb_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2fW3Rsp_x7"
      },
      "source": [
        "\r\n",
        "Try different ANN models and train them on your training set.\r\n",
        "You can play with:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> 3.1. Number of layers.\r\n",
        "\r\n",
        "> 3.2. Activation functions of the layers.\r\n",
        "\r\n",
        ">3.3. Number of neurons in the layers.\r\n",
        "\r\n",
        ">3.4. Different batch sizes during training.\r\n",
        "\r\n",
        "Let's start with a model from the checkpoint that worked well with the MNIST, just as a starting point: 3 dense layers with 128, 64, and 10 neurons, with batch size set to 128, and then tweak the number of layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq0wYQyKp97D",
        "outputId": "c7259aa5-6b38-4013-937a-278121489670"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.44902896881103516\n",
            "Test accuracy: 0.8364999890327454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwr7zzrwqjzK"
      },
      "source": [
        "The above model had a decent accuracy score and does not seem to suffer from overfitting. I'll add another layer and see how they compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_7_Xtaeqe5-",
        "outputId": "8ce23cb2-bd09-4561-dd08-6189c047f39b"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 4 dense layers with 128, 64, 58, 10 neurons\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(58, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.42645224928855896\n",
            "Test accuracy: 0.8454999923706055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrZcb8WCqruh"
      },
      "source": [
        "The results for models 1 and 2 are incredibly similar for both test and training set accuracy scores. I'll add a fifth layer and see how that stacks up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "581J7ugQqpbm",
        "outputId": "6a0b10e4-f2e3-4d08-b591-fdd2b551e0b8"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 5 dense layers with 128, 100, 64, 58, 10 neurons\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(100, activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(58, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.38783755898475647\n",
            "Test accuracy: 0.8615000247955322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1MZBaLiqysF"
      },
      "source": [
        "Adding the fifth layer very slightly increased the test score's accuracy but the training score lowered. I'll stick with 3 layers since the accuracy scores are all similar and this way there are fewer parameters to estimate.\r\n",
        "\r\n",
        "Now, I want to see how varying the activation function changes the results. I'll compare relu, tanh, and sigmoid activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uy_8kmYq-jJ"
      },
      "source": [
        "Using ReLU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf4iHbRfqwMZ",
        "outputId": "1010c1eb-6e0a-4be1-dcd3-25da69aa4e0e"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.42280054092407227\n",
            "Test accuracy: 0.8490999937057495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L2gX3frq68d"
      },
      "source": [
        "Using tanh activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhukwnymq17R",
        "outputId": "8abe7ebb-ea9c-4a9a-aa5f-07dd1b5b263f"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to tanh\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"tanh\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.4173124432563782\n",
            "Test accuracy: 0.8492000102996826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxdOEV02rExG"
      },
      "source": [
        "Using sigmoid activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kOzZ8CarCG-",
        "outputId": "db9164ec-eb97-4228-e714-63e2ed8ddee3"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to sigmoid\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"sigmoid\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.499099999666214\n",
            "Test accuracy: 0.8213000297546387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYWa_ALLrM08"
      },
      "source": [
        "Varying the activation functions did produce slighly different results. The model that used the ReLU activation function had a very slightly higher accuracy score for the test set than the one that used the tanh activation function. These two models also had very similar scores for the training sets. The model with the sigmoid activation function had a lower accuracy score for the test set but a higher accuracy score for the training set. None of the three models appear to suffer from overfitting. The model that used the ReLU activation function did perform the best of the three, but only slightly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJcW4tdRrPbG"
      },
      "source": [
        "Now, let's vary the number of neurons in each layer. I'll start with the model that has thus far performed the best and then vary the number of neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et-tHYP-rHLl",
        "outputId": "251b93a5-fbbb-4862-b4f3-af49628962bd"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.4214297831058502\n",
            "Test accuracy: 0.8508999943733215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rITKT-8PrRqw",
        "outputId": "2ef0b640-9cce-4679-8b2f-5b1f237c9092"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 100, 50, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(100, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(50, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.4433915615081787\n",
            "Test accuracy: 0.8460000157356262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJgSjIPqra-4",
        "outputId": "0d7953b4-e9e6-49e4-ca3e-57ca921e82ac"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 1000, 500, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(1000, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(500, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.39236685633659363\n",
            "Test accuracy: 0.861299991607666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKGK7ZfUrf3m",
        "outputId": "01f6467f-4d86-4540-c27c-524aa780048d"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 1500, 800, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(1500, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(800, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.3887951076030731\n",
            "Test accuracy: 0.8629000186920166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HASFoTQOroO3"
      },
      "source": [
        "I was able to achieve a slightly higher accuracy when changing the number of neurons in the layers. Specifically, increasing the neurons in each layer to 1000, 500, and 10 resulted in a slightly higher accuracy score (86 %) but a slightly lower score for the training set. However, this model took longer to run and the 1% increase in accuracy is not worth the extra computational power needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJRhxdPor3wj"
      },
      "source": [
        "Finally, I will tinker with the batch sizes to see how different models compare. I'll use a lower batch size number, a higher batch size, and the full sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLsoG1yAr534",
        "outputId": "9743bbda-6ac3-4352-94b2-b21624863353"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 128 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.42241567373275757\n",
            "Test accuracy: 0.8482000231742859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPlv1ojos-_J",
        "outputId": "885a445e-1818-4138-a7e9-c6c1e24bd87e"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#set 300 as mini batch size\r\n",
        "model.fit(X_train, Y_train, batch_size=300, epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.47558721899986267\n",
            "Test accuracy: 0.8331000208854675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LQ2gvqys_JK",
        "outputId": "a5fadada-1075-437e-d3b5-713864316210"
      },
      "source": [
        "#build the model\r\n",
        "model = Sequential()\r\n",
        "# add 3 dense layers with 128, 64, 10 neurons with activation set to 'relu'\r\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\r\n",
        "model.add(Dense(64, activation=\"relu\"))\r\n",
        "model.add(Dense(10, activation=\"softmax\"))\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "#use full sample\r\n",
        "model.fit(X_train, Y_train, batch_size=X_train.shape[0], epochs=20, verbose=False)\r\n",
        "#evaluate the model\r\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "print('Test score:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 2.0873217582702637\n",
            "Test accuracy: 0.3075999915599823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJrNXAzZtGBT"
      },
      "source": [
        "Varying the batch size led to interesting results. Using the full sample produced a much lower accuracy for the test set (33.8%). Using a lower batch number (8) led to the highest accuracy for the test set thus far (89%) but also the lowest accuracy for the training set (suggesting that there might be some overfitting happening with that model).\r\n",
        "\r\n",
        "Taking computational time and test/training accuracy rates into consideration, I would argue that the model with 3 dense layers, with 128, 64, and 10 neurons, with batch size set to 128, and with the ReLU activation function performed the best. This model does not seem to suffer from overfitting (based on the difference between the test/training set accuracy scores) and is fairly accuracy (85%). While a few models did have slightly higher accuracy scores, such models either had lower accuracy rates for the training sets (indicating potential overfitting issues) or took significantly longer to run.\r\n",
        "\r\n",
        "In the checkpoints, we were able to achieve accuracy rates of 97% for the MNIST dataset using ANNs. In this challenge, I was only able to achieve accuracy rates of 80-88% even when varying the activation function, the number of layers, neurons, batch sizes, etc. This seems to indicate that it is more challenging to accurately model the Fashion MNIST data than the MNIST data. I could keep tuning the hyperparameters, but perhaps another deep learning approach could achieve even better accuracy rates than these ANN models were able to. 85% accuracy is fine but there is certainly room for improvement here."
      ]
    }
  ]
}